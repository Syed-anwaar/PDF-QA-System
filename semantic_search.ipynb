{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5788564c-db7e-4efc-a94e-0da15c5bfce1",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a209834-3396-43b2-9f9a-688cedd0eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76a3a2-e6f3-4269-8b5d-46080c4ed1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e27f1de4-1511-4368-b420-ad9e07be3cdf",
   "metadata": {},
   "source": [
    "# STEP1- Import Libraries\n",
    "# Purpose: Load all Python libraries needed for PDF reading, NLP, embeddings, and search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b772952f-bee1-4961-8120-9db3f7076407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shaheer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pypdf import PdfReader\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39a2f1-0f0d-4c2e-acd5-72c6cd4b53b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1648a97-b640-4921-80f5-993208ad67b5",
   "metadata": {},
   "source": [
    "# STEP2- PDF Loader\n",
    "# Purpose: Upload or read the PDF and extract all text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd19acb-1592-4e07-a870-48fce4575477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF loaded successfully\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"data/sample.pdf\"  # Replace with your PDF path\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "pdf_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        pdf_text += text + \" \"\n",
    "\n",
    "print(\"PDF loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5bc52-b03e-40ad-8000-c27e4cf3b455",
   "metadata": {},
   "source": [
    "# STEP3- Preprocess PDF Text\n",
    "# Purpose: Clean and split PDF into chunks for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b2a8024-c7d8-4c40-a55b-47319f288d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 36\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=150, overlap=30):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "documents = chunk_text(pdf_text)\n",
    "df = pd.DataFrame(documents, columns=[\"text\"])\n",
    "print(\"Total chunks created:\", len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba084e-6046-4dac-b3ea-44778d53d847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d98018d5-6788-40b7-a600-b149e390ee87",
   "metadata": {},
   "source": [
    "# STEP4- Load Pretrained Embedding Model\n",
    "# Purpose: Prepare embedding model for semantic similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2334cd2f-b9d0-4132-a755-e169fb233320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f4276-0817-4665-955e-ebf9c5c1ec4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88b63ff2-d156-46b8-bdcf-039be12d6918",
   "metadata": {},
   "source": [
    "# STEP5- Create Embeddings\n",
    "# Purpose: Convert PDF text chunks into numeric vectors for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15db6723-4b6d-4039-b497-7bbb659d6670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdafddcbb6343adb965c8ecf5d54b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated: (36, 384)\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(df[\"text\"].tolist(), show_progress_bar=True)\n",
    "print(\"Embeddings generated:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d7a8d-386d-4275-9954-c5e1e9343700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02a3435b-ab11-44db-b961-e132eded623b",
   "metadata": {},
   "source": [
    "# STEP6- Create FAISS Index\n",
    "\n",
    "# Purpose: Store embeddings in FAISS for fast nearest-neighbor search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b86e5ea-da2d-4e0c-b1b7-b2c34f878231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF chunks indexed in FAISS: 36\n"
     ]
    }
   ],
   "source": [
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "print(\"PDF chunks indexed in FAISS:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b6e692-4d13-4ef5-8143-67e79201a803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e83b4015-3eb9-4190-90f6-f0a47a4dbd9e",
   "metadata": {},
   "source": [
    "# STEP7 - Semantic Search Function\n",
    "\n",
    "# Purpose: Find the top-k relevant PDF chunks based on question similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88f30be6-2f89-4af3-8d50-20df1fb34f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, top_k=3):\n",
    "    query_embedding = model.encode([query])\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return [df.iloc[i][\"text\"] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f5744-92fc-4186-8d9d-8f7688d8029c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a69bfbeb-cea0-4c99-9f45-effe9b0aca23",
   "metadata": {},
   "source": [
    "# Step 8- Answer Generation Logic (KEY STEP)\n",
    "# purpose: Extracts relevant PDF content and returns clean bullet-point answers based on the user’s question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e81f809-983b-4f6c-ae2e-a0eb3faa111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, retrieved_chunks, max_points=3):\n",
    "    text = \" \".join(retrieved_chunks)\n",
    "    lines = text.replace(\"•\", \"\\n\").replace(\"\\r\", \"\\n\").split(\"\\n\")\n",
    "    bullets = []\n",
    "\n",
    "    query_words = [w.lower() for w in question.split() if len(w) > 3]\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        low = line.lower()\n",
    "        words = line.split()\n",
    "\n",
    "        # Skip headings/faculty/unit/short gibberish lines\n",
    "        if (\n",
    "            len(line) < 15 or\n",
    "            \"professor\" in low or\n",
    "            \"dept\" in low or\n",
    "            \"unit\" in low or\n",
    "            sum(len(w) == 1 for w in words) > len(words) * 0.6\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        # Split line into sentences\n",
    "        sentences = re.split(r'(?<=[.!?]) +', line)\n",
    "        for sent in sentences:\n",
    "            sent = sent.strip()\n",
    "            if len(sent) > 20 and any(word in sent.lower() for word in query_words):\n",
    "                bullets.append(f\"- {sent}\")\n",
    "            if len(bullets) == max_points:\n",
    "                break\n",
    "        if len(bullets) == max_points:\n",
    "            break\n",
    "\n",
    "    if bullets:\n",
    "        return \"\\n\".join(bullets)\n",
    "    else:\n",
    "        return \"Answer not clearly found in the document.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7429b860-d353-495b-99b6-208e72f2fef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bb2e1cf-e24a-474b-9e02-674302ac9fc1",
   "metadata": {},
   "source": [
    "# STEP 9- Combine Search + Answer\n",
    "\n",
    "# Purpose: End-to-end answer from PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb01c69-7fcf-407b-9391-a8716a91ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_pdf_search(question):\n",
    "    chunks = semantic_search(question, top_k=5)\n",
    "    query_words = [w.lower() for w in question.split() if len(w) > 3]\n",
    "\n",
    "    candidate_sentences = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        sentences = re.split(r'(?<=[.!?]) +', chunk)\n",
    "        for sent in sentences:\n",
    "            sent_low = sent.lower().strip()\n",
    "            # Skip headings/questions\n",
    "            if len(sent_low) < 30:\n",
    "                continue\n",
    "            if any(word in sent_low for word in query_words):\n",
    "                # skip if sentence ends with a question mark (likely a heading)\n",
    "                if sent_low.endswith('?'):\n",
    "                    continue\n",
    "                candidate_sentences.append((sent.strip(), sum(word in sent_low for word in query_words)))\n",
    "\n",
    "    if not candidate_sentences:\n",
    "        return \"Answer not clearly found in the document.\"\n",
    "\n",
    "    # Sort sentences by keyword matches\n",
    "    candidate_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Take top 2–3 sentences\n",
    "    bullets = [f\"- {s[0]}\" for s in candidate_sentences[:3]]\n",
    "    return \"\\n\".join(bullets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cae5eb-ba28-4c6f-8818-7e883826dcae",
   "metadata": {},
   "source": [
    "# STEP10- Interactive Question Loop\n",
    "# purpose: Continuously takes user questions and displays answers, enabling real-time interaction with the PDF content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1963763-8d9e-4030-9614-f93b2f107c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b27de7-07d6-42b9-925d-452d2aeaf03f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Question Answering System\n",
      "Type 'exit' to stop\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question from the PDF:  what is Need for Cyber Law?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " - Need for Cyber Law(cont’d) • Cyberspace offers never-seen-before economic efficiency.\n",
      "- Need for Cyber Law(cont’d) • A software source code worth crores of rupees or a movie can be pirated across the globe within hours of their release.\n",
      "- computer trespassing • Computer vandalism • Transmission of harmful programmes • Stealing secret information & data • Copy rights Against Property Against government • * Hacking government website • * Cyber extortion • * Cyber terrorism • * Computer viruses Some other crimes: • Logic bombs -virus, worms, Trojan horse, email bombing • Spamming - E-mail abuse Need for Cyber Law There are various reasons why it is extremely difficult for conventional law to cope with cyberspace.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question from the PDF:  what is Cyber Laws of India?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " - Hence the need for Cyberlaws in India.\n",
      "- Initially it may seem that Cyberlaws is a very technical field and that it does not have any bearing to most activities in Cyberspace.\n",
      "- Cyber Laws of India • In Simple way we can say that cyber crime is unlawful acts wherein the computer is either a tool or a target or both.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question from the PDF:  Why Cyberlaw in India ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " - Hence the need for Cyberlaws in India.\n",
      "- • Cyberlaw is important because it touches almost all aspects of transactions and activities on and concerning the Internet, the World Wide Web and Cyberspace.\n",
      "- Initially it may seem that Cyberlaws is a very technical field and that it does not have any bearing to most activities in Cyberspace.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question from the PDF:  Does Cyberlaw concern me ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " - • Yes, Cyberlaw does concern you.\n",
      "- • Cyberlaw is important because it touches almost all aspects of transactions and activities on and concerning the Internet, the World Wide Web and Cyberspace.\n",
      "- Initially it may seem that Cyberlaws is a very technical field and that it does not have any bearing to most activities in Cyberspace.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"PDF Question Answering System\")\n",
    "print(\"Type 'exit' to stop\\n\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"Ask a question from the PDF: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    answer = direct_pdf_search(query)\n",
    "    print(\"\\nAnswer:\\n\", answer)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da1e5c-cc48-4435-9ad7-46a50bb51d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35926d3e-f74c-44cd-8e90-a9a34a0cf630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6387656-267c-4364-b3bf-be02d96eadd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44241d9f-643f-4c2c-869d-dda930c1cc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492a5df-8eac-4dca-a635-1e8aa0559e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925eb907-a56d-4501-b446-db3aa26d6f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0732b416-c2e9-40ac-9912-ef96dbbe9f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13bcbf2-9447-402a-a4c7-91721f3ae33b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
